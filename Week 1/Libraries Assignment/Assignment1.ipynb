{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7IlzQhajs71"
      },
      "source": [
        "###Question 1:\n",
        "Generate a dataset for linear regression with 1000 samples, 5 features and single target.\n",
        "\n",
        "Visualize the data by plotting the target column against each feature column. Also plot the best fit line in each case.\n",
        "\n",
        "Hint : search for obtaining regression line using numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X4-07o0-eHZU"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_regression \u001b[38;5;28;01mas\u001b[39;00m mr\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression as mr\n",
        "X,y = mr(n_samples=1000, n_features=5, noise=0)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "def plot_data_fit(X, y, feature_name):\n",
        "  m, b = np.polyfit(X, y, 1)  # Linear fit (degree 1)\n",
        "  plt.scatter(X, y)\n",
        "  plt.plot(X, m * X + b, color='red')\n",
        "  plt.xlabel(feature_name)\n",
        "  plt.ylabel(\"Target (y)\")\n",
        "  plt.title(f\"Target vs Feature {feature_name} with Best Fit Line\")\n",
        "  plt.show()\n",
        "\n",
        "for i in range(X.shape[1]):\n",
        "  plot_data_fit(X[:, i], y, f\"X{i+1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOGDTvDVd57W"
      },
      "source": [
        "### Question 2:\n",
        "Make a classification dataset of 1000 samples with 2 features, 2 classes and 2 clusters per class.\n",
        "Plot the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DspQLHVeeH01"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, n_clusters_per_class=2, n_redundant= 0, n_repeated=0)\n",
        "\n",
        "features = X\n",
        "target = y\n",
        "\n",
        "colors = ['blue' if label == 0 else 'red' for label in target]\n",
        "\n",
        "plt.scatter(features[:, 0], features[:, 1], c=colors)\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.title(\"Classification Dataset with 2 Features and 2 Classes (2 Clusters per Class)\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ghM2NebJXtR"
      },
      "source": [
        "### Question 3:\n",
        "Make a clustering dataset with 2 features and 4 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjjsnbxieIZN"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "\n",
        "X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=4, random_state=0)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.title(\"Clustering Dataset with 2 Features and 4 Clusters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eskxgE9T1jh2"
      },
      "source": [
        "## Question 4\n",
        "Go to the website https://www.worldometers.info/coronavirus/ and scrape the table containing covid-19 infection and deaths data using requests and BeautifulSoup.  Convert the table to a Pandas dataframe with the following columns : Country, Continent, Population, TotalCases, NewCases, TotalDeaths, NewDeaths,TotalRecovered, NewRecovered,  ActiveCases.\n",
        "\n",
        "*(<b>Optional Challenge :</b> Change the data type of the Columns (Population ... till ActiveCases) to integer. For that you need to remove the commas and plus signs. You may need to use df.apply() and pd.to_numeric() . Take care of the values which are empty strings.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7fs4Th9eI6W"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://www.worldometers.info/coronavirus/'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "table = soup.find('table', id='main_table_countries_today')\n",
        "\n",
        "headers = [th.text.strip() for th in table.find('thead').find_all('th')]\n",
        "\n",
        "rows = table.find('tbody').find_all('tr')\n",
        "\n",
        "data = []\n",
        "for row in rows:\n",
        "    cells = [td.text.strip() for td in row.find_all('td')]\n",
        "    data.append(cells[1:9])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data, columns=headers[1:9])\n",
        "\n",
        "#i dont know if its working\n",
        "def convert_to_int(col):\n",
        "    \"\"\"\n",
        "    Converts a Pandas Series to integers, handling commas, plus signs, and empty strings.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return pd.to_numeric(col.str.replace(',', ''), errors='coerce')\n",
        "    except:\n",
        "        return col\n",
        "for col in df.columns[2:]:\n",
        "    df[col] = df[col].apply(convert_to_int)\n",
        "df.dropna()\n",
        "\n",
        "print(df.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhHpN4yCxn-H"
      },
      "source": [
        "# Question 5\n",
        "\n",
        "Generate an imbalanced classification dataset using sklearn of 1000 samples with 2 features, 2 classes and 1 cluster per class. Plot the data. One of the class should contain only 5% of the total samples. Confirm this either using numpy or Counter. Plot the data.\n",
        "\n",
        "Now oversample the minority class to 5 times its initial size using SMOTE. Verify the number. Plot the data.\n",
        "\n",
        "Now undersample the majority class to 3 times the size of minority class using RandomUnderSampler. Verify the number. Plot the data.\n",
        "\n",
        "Reference : Last markdown cell of the examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLKcLL42lCa2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define dataset parameters\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "n_classes = 2\n",
        "n_clusters_per_class = 1\n",
        "\n",
        "# Generate the imbalanced dataset (majority class: 95%, minority class: 5%)\n",
        "X, y = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_redundant= 0, n_repeated=0,\n",
        "                           weights=[0.95, 0.05], n_clusters_per_class=n_clusters_per_class, random_state=0)\n",
        "\n",
        "# Verify class distribution using Counter\n",
        "class_counts = Counter(y)\n",
        "print(\"Original class distribution:\", class_counts)\n",
        "minority_class = min(class_counts, key=class_counts.get)\n",
        "minority_count = class_counts[minority_class]\n",
        "\n",
        "# Plot the imbalanced dataset\n",
        "colors = ['blue' if label == 0 else 'red' for label in y]\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colors, label='Imbalanced Data')\n",
        "plt.title(\"Imbalanced Classification Dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Oversample the minority class using SMOTE (5 times)\n",
        "oversample = SMOTE(sampling_strategy={minority_class: 5 * minority_count})\n",
        "X_resampled, y_resampled = oversample.fit_resample(X, y)\n",
        "\n",
        "# Verify oversampling result\n",
        "resampled_counts = Counter(y_resampled)\n",
        "print(\"Oversampled class distribution:\", resampled_counts)\n",
        "\n",
        "# Plot the oversampled data\n",
        "colors = ['blue' if label == 0 else 'red' for label in y_resampled]\n",
        "plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, label='Oversampled Data')\n",
        "plt.title(\"Oversampled Classification Dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Undersample the majority class using RandomUnderSampler (3 times minority class size)\n",
        "majority_class = (set(range(n_classes)) - {minority_class}).pop()  # Find the majority class\n",
        "majority_count = class_counts[majority_class]\n",
        "undersample = RandomUnderSampler(sampling_strategy={majority_class: minority_count * 3})  # Undersample majority to 3x minority size\n",
        "X_undersampled, y_undersampled = undersample.fit_resample(X, y)\n",
        "\n",
        "# Verify undersampling result\n",
        "undersampled_counts = Counter(y_undersampled)\n",
        "print(\"Undersampled class distribution:\", undersampled_counts)\n",
        "\n",
        "# Plot the undersampled data\n",
        "colors = ['blue' if label == 0 else 'red' for label in y_undersampled]\n",
        "plt.scatter(X_undersampled[:, 0], X_undersampled[:, 1], c=colors, label='Undersampled Data')\n",
        "plt.title(\"Undersampled Classification Dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_j0Smzgk6mZ"
      },
      "source": [
        "##Question 6\n",
        "\n",
        "Write a Python code to perform data preprocessing on a dataset using the scikit-learn library. Follow the instructions below:\n",
        "\n",
        " * Load the dataset using the scikit-learn `load_iris` function.\n",
        " * Assign the feature data to a variable named `X` and the target data to a variable named `y`.\n",
        " * Create a pandas DataFrame called `df` using `X` as the data and the feature names obtained from the dataset.\n",
        " * Display the first 5 rows of the DataFrame `df`.\n",
        " *  Check if there are any missing values in the DataFrame and handle them accordingly.\n",
        " * Split the data into training and testing sets using the `train_test_split` function from scikit-learn. Assign 70% of the data to the training set and the remaining 30% to the testing set.\n",
        " * Print the dimensions of the training set and testing set respectively.\n",
        " *  Standardize the feature data in the training set using the `StandardScaler` from scikit-learn.\n",
        " *  Apply the same scaling transformation on the testing set.\n",
        " * Print the first 5 rows of the standardized training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCJg725i4xiY"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "feature_names = iris.feature_names\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "if df.isnull().values.any():\n",
        "    print(\"Missing values found!\")\n",
        "else:\n",
        "    print(\"No missing values found!\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set dimensions: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Testing set dimensions: {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "print(\"\\nFirst 5 rows of standardized training set:\")\n",
        "print(pd.DataFrame(X_train_scaled[:5], columns=feature_names))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
